# Awesome Learning Materials for Generative

Hello There,

Thanks for checking out this blog. We can all agree that with the flood of information available today, it can be challenging to find quality content amid the clutter. Since the launch of ChatGPT in December 2020, I have been deeply involved in following and working with Generative AI. In this blog, I have compiled some key articles and videos on the subject.

If you come across any content that you believe should be included in this list, please feel free to reach out to me on https://www.linkedin.com/in/anurag-mishra-660961b7/. So that it can help many AI enthusiast to learn and improve their skills.

I frequently write about developments in Generative AI and Machine learning, so feel free to follow me on Medium (https://medium.com/@anuragmishra_27746)


# Link to Videos/Courses

## Large Language Models Frameworks & Architecture Design

- ``Large Language Models: Foundation Models from the ground up`` https://www.youtube.com/watch?v=W0c7jQezTDw&list=PLTPXxbhUt-YWjMCDahwdVye8HW69p5NYS
- ``Large Language Models: Application through Production`` https://www.youtube.com/watch?v=3H5UC2c0Dyc&list=PLTPXxbhUt-YWSR8wtILixhZLF9qB_1yZm
- ``State of GPT | BRK216HFS`` https://www.youtube.com/watch?v=bZQun8Y4L2A&t=925s
- ``[1hr Talk] Intro to Large Language Models`` https://www.youtube.com/watch?v=zjkBMFhNj_g
- ``Let's reproduce GPT-2 (124M)`` https://www.youtube.com/watch?v=l8pRSuU81PU&t=32s
- ``Let's build the GPT Tokenizer`` https://www.youtube.com/watch?v=zduSFxRajkE&t=13s
- ``The spelled-out intro to neural networks and backpropagation: building micrograd`` https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLXYLzZ3XzIbi4lL43O6fIU_ojuZwBO6vi
- ``What Is LangChain? - LangChain + ChatGPT Overview`` ttps://www.youtube.com/watch?v=_v_fgW2SkkQ&list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5
- ``LangChain for LLM Application Development`` https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/

## Prompt Engineering

- https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/
- https://www.youtube.com/watch?v=k7HaeJs-N-o&t=2454s
- https://www.youtube.com/watch?si=D-OX6iR3upHXOcyC&v=eBjxz7qrNBs&feature=youtu.be

## Advanced RAG Design

- https://www.youtube.com/watch?v=wd7TZ4w1mSw&list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x
- https://youtu.be/8OJC21T2SL4?si=AfpytpVceSJZb0bv
- https://www.youtube.com/watch?v=4EXOmWeqXRc
- https://www.youtube.com/watch?v=L1o1VPVfbb0
- https://www.youtube.com/watch?v=Ya1DhVW9gTo

## Evaluation & Monitoring

- https://www.youtube.com/watch?v=2CIIQ5KZWUM&list=PLNEBgV2Tn1rjy2ADU4Uk8kmuDCdxFpjyj&index=7&t=1748s
- https://youtu.be/mEv-2Xnb_Wk?si=Z2nHIbH4mnUWIuA1 - raga framework
- https://youtu.be/fWC4VxolWAk?si=xSLBTJmZm7M9E1Rc - raga framework v2
- https://www.youtube.com/watch?v=vygFgCNR7WA&list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S

## Agentic Workflow

- https://youtu.be/DWUdGhRrv2c?si=f3kyEFv_rAk1N_oJ
- https://www.youtube.com/watch?v=v9fkbTxPzs0&t=1s
- https://www.youtube.com/watch?v=5h-JBkySK34&list=PLfaIDFEXuae16n2TWUkKq5PgJ0w6Pkwtg
- https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/
- https://www.youtube.com/watch?v=l7lvoiCvcVU
- https://www.youtube.com/watch?v=uRya4zRrRx4 - Plan & Execute using LangGraph
- https://www.youtube.com/watch?v=v5ymBTXNqtk&t=4s - reflection agent
- https://youtu.be/eTCdNjYlgk0?si=gD8BYO23KePrXLvI
- https://youtu.be/EsDzpptLKd8?si=WAGbyeL58MY7HfvI

## Fine-Tuning LLM

- https://www.youtube.com/watch?v=2QRlvKSzyVw&list=PLD80i8An1OEGqqXeNZ5w0IBmeZcxpZEYL
- https://www.youtube.com/watch?v=mYRqvB1_gRk - Talk about throughput & Latency
- https://youtu.be/eC6Hd1hFvos?si=CEyAJH4P6F0qZH_d
- https://www.youtube.com/watch?v=JVrMxTkgoUA
- https://www.youtube.com/watch?v=TIqf4LMNCjU - distilling LLM
- https://www.youtube.com/watch?v=eUuGdh3nBGo - Cuda Programming
- https://www.youtube.com/watch?v=t509sv5MT0w - Explanation of LoRA
- https://www.youtube.com/watch?v=zcMQXID447s - FIne LlaMa w/t sft
- https://www.youtube.com/playlist?list=PLAwxTw4SYaPnFKojVQrmyOGFCqHTxfdv2 - intro course on cuda programmin
- https://www.youtube.com/watch?v=L-hRexVa32k - Time LLM

## Knowledge Graph with LLM

- https://www.deeplearning.ai/short-courses/knowledge-graphs-rag/
- https://www.youtube.com/watch?v=Sra-1xhNn28&list=PL9Hl4pk2FsvX-5QPvwChB-ni_mFF97rCE
- https://youtu.be/Hg4ahTQlBm0?si=Mwbu5C1FQvtj2nLo
- https://youtu.be/1sRgsEKlUr0?si=qFscURiTc3m5h3u3
- https://youtu.be/Fh-alMQ7h_U?si=NEIqV5gNeODV_ghZ
- https://www.youtube.com/watch?v=sK61Y22wvr8
- https://www.youtube.com/live/RmfFyuYki0g

# Articles 

## Large Language Models & Architecture Design

- https://mychen76.medium.com/state-of-function-calling-in-llm-bc3aa37decb4
- https://medium.com/@vinusebastianthomas/document-chains-in-langchain-d33c4bdbabd8

## Prompt Engineering

- https://x.com/moritzkremb/status/1766486621434863743?s=35
- https://hamel.dev/blog/posts/prompt/
- https://www.promptingguide.ai/

## RAG Design and Tips

- https://medium.com/codex/build-end-to-end-rag-pipeline-with-monitoring-and-evaluation-using-langchain-azure-ai-search-6f190fffab2a
- https://medium.com/@anuragmishra_27746/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d
- https://medium.com/@anuragmishra_27746/optimizing-rags-overcoming-architecture-hurdles-for-peak-performance-part-1-8b5493b6a114
- https://research.trychroma.com/evaluating-chunking
- https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/
- https://medium.com/@sgreenman/best-practices-for-llm-optimization-for-call-and-message-compliance-prompt-engineering-rag-and-45ccca32ff17
- https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product
- https://portkey.ai/blog/implementing-frugalgpt-smarter-llm-usage-for-lower-costs/
- https://medium.aiplanet.com/retrieval-augmented-pipeline-with-actions-using-nemo-gaurdrails-447b84a5334b?gi=1481321f389b
- https://towardsdatascience.com/demystifying-ndcg-bee3be58cfe0?gi=7786cbf79971

## Agentic Workflow

- https://medium.com/@anuragmishra_27746/future-of-coding-multi-agent-llm-framework-using-langgraph-092da9493663
- https://medium.com/@anuragmishra_27746/practical-hands-on-with-langchain-expression-language-lcel-for-building-langchain-agent-chain-2a9364dc4ca3
- LCEL language - https://www.pinecone.io/learn/series/langchain/langchain-expression-language/
- https://mer.vin/2024/01/langgraph-agents/ - Code to develop multi-agent
- https://python.langchain.com/docs/langgraph#end - Function Definitions
- https://python.langchain.com/docs/modules/model_io/chat/function_calling - Function calling features
- [Parallel tool use | ü¶úÔ∏èüîó Langchain](https://python.langchain.com/docs/use_cases/tool_use/parallel) - Run parallels calls
- https://medium.com/@manoranjan.rajguru/search-product-catalog-images-using-azure-search-and-openai-with-langchain-3a844bc5c27c - shopping agent
- https://python.langchain.com/docs/expression_language/cookbook/code_writing - Run python code
- https://medium.com/@kbdhunga/langgraph-multi-agent-collaboration-explained-c0500b0f2e61

## Evaluation & Monitoring Framework

- https://blog.relari.ai/a-practical-guide-to-rag-evaluation-part-2-generation-c79b1bde0f5d
- https://arize.com/blog/evaluate-rag-with-llm-evals-and-benchmarking/
- https://hamel.dev/blog/posts/evals/?s=35
- https://docs.smith.langchain.com/concepts/evaluation
- https://medium.com/@jeffreyip54/how-to-evaluate-llm-applications-3582505e14e3
- https://blog.research.google/2024/01/introducing-aspire-for-selective.html
- https://huggingface.co/blog/evaluating-mmlu-leaderboard?ref=radekosmulski.com
- https://txt.cohere.com/evaluating-llm-outputs/
- https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG
- Depends on the use case and goal. But a combination of these is also reasonable. I like starting out with a small evaluation dataset (even better if it exists) and trying standard metrics on the task and if I need scaling and robust evaluation (e.g., coherence, creativity, etc.), use LLM-powered evaluation with a well-defined rubric. Experimenting is key to figuring out what works best for the use case.
- https://twitter.com/aparnadhinak/status/1748413460575089138 - thread for qualitative one
- https://cookbook.openai.com/examples/evaluation/how_to_eval_abstractive_summarization
- https://www.kaggle.com/code/nischaydnk/llm-finetune-token-classification-h2o-danube-1-8b - LLM Finetuning
- https://huggingface.co/blog/evaluating-llm-bias - Evaluate on Toxicity and
- https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
- https://huggingface.co/learn/cookbook/llm_judge
- https://medium.com/@vipra_singh/building-llm-applications-retrieval-search-part-5-c83a7004037d - evaluation approaches
- https://arize.com/blog/evaluate-rag-with-llm-evals-and-benchmarking/
- https://docs.smith.langchain.com/cookbook/testing-examples - Note down best practices in langchain
- https://insidelearningmachines.com/precisionk_and_recallk/ - precision & recall
- https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c
- [Fixing Hallucinations in LLMs. Why LLMs hallucinate, approaches for‚Ä¶ | by Sergei Savvov | Better Programming](https://betterprogramming.pub/fixing-hallucinations-in-llms-9ff0fd438e33)
- https://swaroopch.com/blog/mastering-llms/#evals-is-a-must
- https://towardsdatascience.com/judge-an-llm-judge-a-dual-layer-evaluation-framework-for-continous-improvement-of-llm-apps-7450d0e81e17

## Fine-Tuning LLM

- https://medium.com/@saha.saumajit/unlocking-the-secrets-of-llm-alignment-your-quick-guide-2e286123c28c
- https://github.com/huggingface/blog/blob/main/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md
- https://www.linkedin.com/pulse/infrastructure-requirements-llms-arivukkarasan-raja-j0acc/
- https://balabala76.medium.com/llama-2-fine-tuning-using-azure-machine-learning-in-parallel-cf9720d1d60e
- Casual Attention - https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention
- https://huggingface.co/blog/hf-bitsandbytes-integration
    
    **Inference Time** 
    
    - https://betterprogramming.pub/speed-up-llm-inference-83653aa24c47 - speed up inference
    - https://pytorch.org/blog/accelerating-generative-ai-3/?utm_content=277363729&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366
    - [LLM Inference Performance Engineering: Best Practices | Databricks Blog](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices)
    - https://colab.research.google.com/drive/15iNsv2rJzV2_srQajsx-aRqRoeWCfEiW#scrollTo=RuCGgmlawA8j
    - https://huggingface.co/blog/llama2 - llama 2 inference
    - Feature of vLLM | wwwerr
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/6476e561-ab64-4c38-a61b-5e9144103956/6ec1c657-ac4f-4bc5-8a26-5c22241bb4e6/Untitled.png)
    
    - [https://medium.com/@premsaig1605/deciphering-the-giants-a-comprehensive-comparison-of-text-generation-interface-tgi-and-vllm-26da3976d0e9#:~:text=TGI is celebrated for its,ideal for resource-intensive tasks](https://medium.com/@premsaig1605/deciphering-the-giants-a-comprehensive-comparison-of-text-generation-interface-tgi-and-vllm-26da3976d0e9#:~:text=TGI%20is%20celebrated%20for%20its,ideal%20for%20resource%2Dintensive%20tasks).
    - https://medium.com/@rohit.k/tgi-vs-vllm-making-informed-choices-for-llm-deployment-37c56d7ff705
    - https://blog.truefoundry.com/deploying-llms-at-scale/
    - https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/how-to-evaluate-llms-a-complete-metric-framework/
    - https://blog.truefoundry.com/benchmarking-mistral-7b/
    - https://blog.vllm.ai/2023/06/20/vllm.html
    - https://huggingface.co/blog/llama2 - llama 2 inference
    - https://x.com/jeremyphoward/status/1757145521507180546?t=qP0zaikKTF4PY94ydox5vw&s=35 - cuda in python
    - https://t.co/w2d3llNUwl - fine tuning
    - https://github.com/Troyanovsky/Local-LLM-Comparison-Colab-UI - Github repo of open-source LLM model
    
    **Training Method** 
    
    - https://www.tensoic.com/blog/kannada-llama/
    - https://huggingface.co/docs/trl/sft_trainer
    - [https://medium.com/@parikshitsaikia1619/mistral-mastery-fine-tuning-fast-inference-guide-62e163198b06#:~:text=vLLM is a fast and,and value memory with PagedAttention](https://medium.com/@parikshitsaikia1619/mistral-mastery-fine-tuning-fast-inference-guide-62e163198b06#:~:text=vLLM%20is%20a%20fast%20and,and%20value%20memory%20with%20PagedAttention).
    - [https://medium.com/@sujathamudadla1213/difference-between-trainer-class-and-sfttrainer-supervised-fine-tuning-trainer-in-hugging-face-d295344d73f7#:~:text=Use Trainer%3A If you have,experience with efficient memory usage](https://medium.com/@sujathamudadla1213/difference-between-trainer-class-and-sfttrainer-supervised-fine-tuning-trainer-in-hugging-face-d295344d73f7#:~:text=Use%20Trainer%3A%20If%20you%20have,experience%20with%20efficient%20memory%20usage).
    - https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_can-a-2b-llm-outperform-mistral-ai-7b-or-activity-7161033776964247555-vd6N?utm_source=share&utm_medium=member_android
    - https://blog.allenai.org/olmo-open-language-model-87ccfc95f580 - Training blog from OLMO model
    - https://huggingface.co/blog/dpo-trl - DPO (Direct Preference Optimization)
    - https://tanaymeh.github.io/blog/2024/02/08/p7.html - Blog for training LLM
    - https://www.linkedin.com/posts/sanyambhutani_llm-fine-tuning-benchmarks-super-excited-activity-7163924118411710464-gbQU?utm_source=share&utm_medium=member_android - benchmarking
    - https://www.together.ai/blog/finetuning
    
    **Data Preparation**
    
    - https://www.realworldml.xyz/blog/llms-in-real-world-projects
    - https://www.philschmid.de/fine-tune-llms-in-2024-with-trl
    - https://eugeneyan.com/writing/synthetic/ - Generating Synthesis Data

## Knowledge Graph with LLM

- https://medium.com/microsoftazure/introducing-graphrag-with-langchain-and-neo4j-90446df17c1e
- https://neo4j.com/blog/unifying-llm-knowledge-graph/
- https://neo4j.com/developer-blog/global-graphrag-neo4j-langchain/
- https://neo4j.com/blog/graphrag-genai-googlecloud/
- https://neo4j.com/developer-blog/graphrag-llm-knowledge-graph-builder/
- https://neo4j.com/labs/genai-ecosystem/llm-graph-builder/
- [https://neo4j.com/developer-blog/graphrag-ecosystem-tools](https://neo4j.com/developer-blog/graphrag-ecosystem-tools/)/
- https://medium.com/neo4j/from-zero-to-graphrag-in-5-minutes-4ffcfcb4ebc2
- https://medium.com/neo4j/graph-based-metadata-filtering-for-improving-vector-search-in-rag-applications-47fd2efcfc0a
- https://gist.github.com/jexp/74bd5a43305550236321eab8f0c723c0
- https://medium.com/@shubham.shardul2019/end-to-end-multimodal-knowledge-graph-creation-from-texts-and-images-querying-in-natural-a28fa2053856
- https://medium.com/@jim.mchugh/hierarchies-graph-databases-e2d7d6c8dd83
- https://www.kaggle.com/code/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk

# Github Repos

- https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/base.ipynb
- https://github.com/microsoft/graphrag
- https://github.com/Azure-Samples/azure-search-performance-testing
- https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/high-level.ipynb
- https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/force-calling-a-tool-first.ipynb
- https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/dynamically-returning-directly.ipynb
- https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/managing-agent-steps.ipynb
- https://github.com/langchain-ai/langgraph/blob/main/examples/chatbot-simulation-evaluation/agent-simulation-evaluation.ipynb
- https://colab.research.google.com/drive/15iNsv2rJzV2_srQajsx-aRqRoeWCfEiW#scrollTo=RuCGgmlawA8j - vLLM benchmarking

## 

- https://arxiv.org/pdf/2401.07103.pdf - survey on LLM evaluation
- https://arxiv.org/pdf/2212.14024 - Demonstrate - Search - Predict Paper
- https://arxiv.org/pdf/2310.03714 - DsPy Paper
- https://arxiv.org/abs/2312.13010 - AgentCoder Paper
- [https://arxiv.org/abs/2401.13178#:~:text=Evaluating large language models](https://arxiv.org/abs/2401.13178#:~:text=Evaluating%20large%20language%20models%20) (LLMs,evaluation%20process%20presents%20substantial%20challenges - Evaluating Multiagent Framework
- https://arxiv.org/abs/2403.10482
- https://arxiv.org/abs/2403.10131
- https://arxiv.org/pdf/2406.06608 - Prompt Survey
- https://arxiv.org/abs/2402.09171
- https://arxiv.org/pdf/2310.10501 - nemo guardrails paper
- https://arxiv.org/abs/2312.13010
- https://arxiv.org/pdf/2401.04088 - Mistral of Experts paper
- https://arxiv.org/abs/2312.16171
- https://arxiv.org/abs/2312.02783?s=35
- https://arxiv.org/abs/2311.12785
- https://arxiv.org/abs/2309.04269
- https://arxiv.org/abs/2309.07864?s=35
- https://arxiv.org/abs/2308.10792
- https://arxiv.org/abs/2307.10169?s=35
- https://arxiv.org/abs//2307.03109
- https://arxiv.org/pdf/2203.11171 - self-consistency prompting
- https://arxiv.org/pdf/2208.07339 - 8 bit matrix multiplication for transformer at scale

# Research Papers

- [2212.14024](https://arxiv.org/pdf/2212.14024) - Demonstrate - Search - Predict Paper
- https://arxiv.org/pdf/2310.03714 - DsPy Paper
- https://arxiv.org/abs/2312.13010 - AgentCoder Paper
- [https://arxiv.org/abs/2401.13178#:~:text=Evaluating large language models](https://arxiv.org/abs/2401.13178#:~:text=Evaluating%20large%20language%20models%20) (LLMs,evaluation%20process%20presents%20substantial%20challenges - Evaluating Multiagent Framework
- https://arxiv.org/abs/2403.10482
- https://arxiv.org/abs/2403.10131
- https://arxiv.org/pdf/2406.06608 - Prompt Survey
- https://arxiv.org/abs/2402.09171
- https://arxiv.org/pdf/2310.10501 - nemo guardrails paper
- https://arxiv.org/abs/2312.13010
- https://arxiv.org/pdf/2401.04088 - Mistral of Experts paper
- https://arxiv.org/abs/2312.16171
- https://arxiv.org/abs/2312.02783?s=35
- https://arxiv.org/abs/2311.12785
- https://arxiv.org/abs/2309.04269
- https://arxiv.org/abs/2309.07864?s=35
- https://arxiv.org/abs/2308.10792
- https://arxiv.org/abs/2307.10169?s=35
- https://arxiv.org/abs//2307.03109
- https://arxiv.org/pdf/2203.11171 - self-consistency prompting
- https://arxiv.org/pdf/2208.07339 - 8 bit matrix multiplication for transformer at scale

# Social Media Posts

- https://x.com/LangChainAI/status/1763970967515353372?t=kbiWSnANLQoNffSZfQbtgA&s=35
- https://www.linkedin.com/posts/gante_up-to-3x-faster-llm-generation-with-no-extra-activity-7153089853918699520-sERg/?utm_source=share&utm_medium=member_android
- https://x.com/karpathy/status/1803963383018066272?s=52
- https://x.com/ashpreetbedi/status/1783926697504419927?t=IF4ILMgZgIy8vnbOTKnGKA&s=35
- https://www.linkedin.com/posts/thom-wolf_this-30-min-read-blog-post-on-how-to-craft-activity-7176481655220502532-BS4A/?utm_source=share&utm_medium=member_android
- https://x.com/DrJimFan/status/1770511408889020510?t=QNpq17WBdBv00CTf5gn79Q&s=35

# GenAI Social Media Influencers (Twitter)

- Harrison Chase (Founder of Langchain)
- Jerry **Jerry Liu** (Founder of Llamaindex)
- Andrej Karpathy (Top DL Researcher)
- Sophia Yang (Head of Relation @MistraAI)
- Mervin Praison - https://x.com/MervinPraison
- **Greg Kamradt**

